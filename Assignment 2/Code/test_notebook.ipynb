{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from math import log2\n",
    "import itertools \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "class e_Support_Vector_Machine:\n",
    "    \"\"\" This is a Support Vector Machine algorithm I've kindly called the emotional Support Vector Machine. \n",
    "    This can be used to for binary classification into 'happy' and 'sad' classes depending on the context of the target variable\n",
    "    (in the case of the assignment, a fire is a 'sad' outcome) - emijis in the relevent plots will support this\n",
    "\n",
    "    There are also variables named after some of my friends dotted throughout this where I feel appropriate eg.:\n",
    "    \n",
    "    'julian_stress' - will be used inplace of C to tune the size of the margin between the support vector and decision boundary. \n",
    "        Low stress would give us a wide margin, giving the system high confidence in its predictions\n",
    "\n",
    "    I am using the sci-kit learn project template as the structure for this class.\n",
    "    \n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    happy_class : str, default='no fire'\n",
    "        A parameter used during plotting to \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    X_ : ndarray, shape (n_samples, n_features)\n",
    "        The input passed during :meth:`fit`.\n",
    "    y_ : ndarray, shape (n_samples,)\n",
    "        The labels passed during :meth:`fit`.\n",
    "    classes_ : ndarray, shape (n_classes,)\n",
    "        The classes seen at :meth:`fit`.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, n_iter, tolerance, C):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.C = C\n",
    "        #self.happy=happy\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Training an SVM is an optimisation problem where the objective is to maximise the distance between a hyperplane and the closest datapoint for any weight and bias (W, b).\n",
    "        The equation for a seperating hyperplane is (W).(X) + b = 0 where W is a weight vector, X is the input vector and b is a bias term.\n",
    "                    \n",
    "        The distance between the hyperplanes is 2/||W|| - so the goal is to minimise the magnitude of W and therefore maximise the \n",
    "        distnce between the hyperplanes with the condition no points lie between the two hyperplanes.\n",
    "        This minimising variable can be re-written as (||W||^2)*(1/2) for convenience, it is now a quadratic with one global minimum.\n",
    "        \n",
    "        So writing the eq. for hyperplanes for the two classes gives:\n",
    "             (W).(Xi) + b = 1 for positive class (yi=1)\n",
    "             (W).(Xi) + b = -1 for negative class (yi=-1)\n",
    "        This can be generalised for both classes as yi is known and can be multiplied by the above equations to give the form:\n",
    "            yi(Xi.W+b)-1=0\n",
    "\n",
    "        We want to minimise ||W|| and maximise b by iterating through possible values for W and keeping the W and b that satisfy the above equation \n",
    "        and picking the W and b that minimise ||W||. This will be done using Stochastic Gradient Descent and the bias term will be evaluated in the W vector.\n",
    "\n",
    "        The cost funciton is described by the equation:\n",
    "        J= (||W||^2)/2  +  (C/N)*SUMALL(maxvalue(0, 1-yi*W*xi)) \n",
    "\n",
    "        The gradient of this cost funciton is :\n",
    "        DJ = 1/N SUMALL(w) if the max(0, 1-yi*W*xi) = 0 and satisfies the general form of the hyperplane equation or\n",
    "        DJ = 1/N SUMALL(w-C*yi*xi) where is does not satify the eqn.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The training input samples.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The target values. An array of int.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        \n",
    "\n",
    "        # Initialise the weight vector with length the same as the number of features being analysed and give each coefficient a value of 1.\n",
    "        W = np.ones(X.shape[1])  \n",
    "\n",
    "        # For each iteration, calculate the weights.\n",
    "        for step in range(self.n_iter):\n",
    "            # For each X value evaluate the gradient at that point with the given weights and subtract the gradient*learning rate from the weights to refine the W vector\n",
    "            for index, value in enumerate(X):\n",
    "                W = W-self.learning_rate*gradient(W, value,  y[index], self.C)\n",
    "        self.W = W\n",
    "        return(self.W)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" A reference implementation of a prediction for a classifier.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            The label for each sample is the label of the closest sample\n",
    "            seen during fit.\n",
    "        \"\"\"\n",
    "        # Check is fit had been called\n",
    "        #check_is_fitted(self, ['X_', 'y_'])\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        output = []\n",
    "        for i in X:\n",
    "            if np.dot(self.W.T,i)>0:\n",
    "                output.append(1)\n",
    "            else:\n",
    "                output.append(-1)\n",
    "        return(output)\n",
    "    \n",
    "    \n",
    "def learning_schedule(t):\n",
    "    # random initialisation for t0, user defined t1\n",
    "    t0, t1 = 5, 50\n",
    "    return t0/(t+t1)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def information_gain(df, target, columns):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for all the columns to be presented at the feature selection screen. \n",
    "    Mean value will be used to bucket the values.\n",
    "    \"\"\"\n",
    "    df_output = pd.DataFrame()\n",
    "    \n",
    "    target_vals = list(set(df[target]))\n",
    "    val1 = target_vals[0]\n",
    "    val2 = target_vals[1]\n",
    "    \n",
    "    df_entropy = -(len(df[df[target]==val1])/len(df))*log2(len(df[df[target]==val1])/len(df)) - (len(df[df[target]==val2])/len(df))*log2(len(df[df[target]==val2])/len(df))\n",
    "    \n",
    "    \n",
    "    for col in columns:        \n",
    "        mean_val = np.mean(df[col])\n",
    "        high_val= df[df[col]>=mean_val]\n",
    "        low_val = df[df[col]<mean_val]\n",
    "        try:\n",
    "            # Some columns like rain have no fires above the mean value so the below equation breaks down - this is a very significant feature to include\n",
    "            if len(set(high_val[target])) ==2 :\n",
    "                high_exp1 = -(len(high_val[high_val[target]==val1]) / len(high_val))*log2(len(high_val[high_val[target]==val1])/len(high_val)) \n",
    "                high_exp2 = -(len(high_val[high_val[target]==val2]) / len(high_val))*log2(len(high_val[high_val[target]==val2])/len(high_val))\n",
    "                high_ent =  high_exp1 + high_exp2\n",
    "            else:\n",
    "                high_ent=0\n",
    "\n",
    "            if len(set(low_val[target])) ==2 :\n",
    "                low_exp1 = -(len(low_val[low_val[target]==val1]) / len(low_val))*log2(len(low_val[low_val[target]==val1])/len(low_val)) \n",
    "                low_exp2 = -(len(low_val[low_val[target]==val2]) / len(low_val))*log2(len(low_val[low_val[target]==val2])/len(low_val))\n",
    "                low_ent =  low_exp1 + low_exp2\n",
    "            else:\n",
    "                low_ent=0\n",
    "\n",
    "            info_gain = df_entropy - (len(high_val)/len(df))*high_ent - (len(low_val)/len(df))*low_ent\n",
    "            df_output = df_output.append([[col,np.round(mean_val,2),np.round(info_gain, 2)]])\n",
    "        except:\n",
    "            pass\n",
    "    df_output = df_output.rename(columns={0:'Column', 1:\"Mean Value\", 2:\"Information Gain\"})\n",
    "    return(df_output)\n",
    "        \n",
    "        \n",
    "\n",
    "def feature_selection(df):\n",
    "    \"\"\"\n",
    "    Allows user input to pick the dependant and independant variables. \n",
    "    Once the dependant variable is chosen the information gain for the independant variables is calculated to help the user pick out useful features. \n",
    "    To calculate information gain variables were binned according to the feature mean value \n",
    "        - this is not useful in the case of categorical data but the user should know that, this is just to assist the feature selection process\n",
    "    \n",
    "    \"\"\"\n",
    "    print(df.dtypes)\n",
    "    target='yes'\n",
    "    #target = input('Pick the target variable')\n",
    "    \n",
    "    df[target] = [x.strip() for x in df[target]]\n",
    "    df[target] = df[target].replace({'no':-1, 'yes':1})\n",
    "    df_cols = df.drop(target ,axis=1)\n",
    "   \n",
    "    ig = information_gain(df, target, df_cols.columns)\n",
    "    info_cols = pd.DataFrame(df_cols.dtypes)\n",
    "    info_cols.reset_index(inplace=True)\n",
    "    info_cols = info_cols.rename(columns={'index':'Column', 0:'Data Type'})\n",
    "    info_cols= info_cols.merge(ig, on='Column').sort_values(\"Information Gain\" ,ascending=False)\n",
    "    print(\"Information gain calculated for bins either side of mean values for each feature\")\n",
    "    print(info_cols)\n",
    "    cols = \"rainfall, humidity, buildup_index, drought_code\"\n",
    "    #cols = input(\"Please enter the desired columns for anaylsis: \")\n",
    "    cols = [x.strip() for x in cols.split(',')]\n",
    "    return target, cols\n",
    "\n",
    "def normalise(df, column):\n",
    "    \"\"\"\n",
    "    Function to normalise the data in the datasets columns - this has a negative impact on the models performance \n",
    "    \"\"\"\n",
    "    return 2*(df[column]-min(df[column]))/(max(df[column]) - min(df[column]))-1\n",
    "    \n",
    "\n",
    "def cost_function(W, X, y, C):\n",
    "    \"\"\"\n",
    "    The cost funciton is described by the equation below and will be evaluated to determine if the model has achieved an acceptably low cost function before the number of iterations has been reached.\n",
    "    J= (||W||^2)/2  +  (C/N)*SUMALL(maxvalue(0, 1-yi*W*xi)) \n",
    "    \"\"\"\n",
    "    for i in range(len(X)):\n",
    "        # Evaluate for the left side of the '+'. Dot product of a vector on itself returns the magnitude\n",
    "        lhs = (1/2) * np.dot(W.T,W)#**0.5\n",
    "        \n",
    "        # Evaluate for right hand side of the '+'\n",
    "        hyper_plane_distance = np.max([0,1-(y[i]*np.dot(X[i],W.T))])\n",
    "        N = len(X)\n",
    "        rhs = (C/N)*np.sum(hyper_plane_distance)\n",
    "        return (lhs+rhs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradient(W, X, y, C):\n",
    "    '''\n",
    "    Calculate the hyperplane distance at a point for given values of W, Xi, yi and return the value for the SVM to evaluate the next values for W.\n",
    "    '''\n",
    "    \n",
    "    grad = np.zeros(len(W))\n",
    "    # Calculate distance to the hyperplane for W, Xi, yi \n",
    "    distance = np.max([0, 1 - y * np.dot(W.T,X)])\n",
    "    \n",
    "    # If the max value of the above is 0 then the point is a support vector and the weights are insightful else decrease the weights by C(yi*Xi)\n",
    "    if distance == 0:\n",
    "        grad = W\n",
    "    else:\n",
    "        grad = W - (C * y * X)\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "def cross_val(clf, X, y, n_folds):\n",
    "    \n",
    "    \"\"\"\n",
    "    SK Learns cross_val_score was not working with my implimentation of the SVM so the below code shuffles and splits the dataset into 9/10 and 1/10 for training and validation. \n",
    "    The first j elements are taken for validation and the remainder are training. Once the first j items have been used for validaiton they are concatenated onto the end of the training set and the next j elements are taken from the top of the training set.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    X,y = shuffle(X,y)\n",
    "    output_scores=[]\n",
    "    for i in range(n_folds):\n",
    "        index_slicer = len(X)//n_folds\n",
    "        X_val, y_val = X[ :index_slicer ], y[ : index_slicer]\n",
    "        X_train, y_train = X[index_slicer: ], y[index_slicer: ]\n",
    "        \n",
    "        # To iterate through the folds of the cross validation, append the first j elements to the end of the array and then slice them off the start.\n",
    "        # By always treating the first j elements as the validation set and the remainder as the training set, I can do n-fold CV without adapting my e_Support_Vector_machine class to accecpt the sklearn implimentation.\n",
    "\n",
    "        X, y =np.concatenate((X_train,X_val)),np.concatenate((y_train,y_val))\n",
    "        #X, y = X[index_slicer: ], y[index_slicer :] \n",
    "        \n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "        clf_predicts = clf.predict(X_val)\n",
    "        f1 = f1_score(y_val, clf_predicts)\n",
    "        print(f'Iteration: {i}.  F1 score: {f1}')\n",
    "        output_scores.append(f1)\n",
    "    print(f'Mean F1 is: {np.mean(output_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes               object\n",
      "year               int64\n",
      "temp               int64\n",
      "humidity           int64\n",
      "rainfall         float64\n",
      "drought_code     float64\n",
      "buildup_index    float64\n",
      "day                int64\n",
      "month              int64\n",
      "wind_speed         int64\n",
      "dtype: object\n",
      "Information gain calculated for bins either side of mean values for each feature\n",
      "          Column Data Type  Mean Value  Information Gain\n",
      "5  buildup_index   float64       16.54              0.30\n",
      "3       rainfall   float64        0.82              0.25\n",
      "4   drought_code   float64       48.54              0.23\n",
      "1           temp     int64       31.91              0.18\n",
      "2       humidity     int64       62.28              0.11\n",
      "6            day     int64       15.69              0.03\n",
      "7          month     int64        7.55              0.03\n",
      "0           year     int64     2011.98              0.00\n",
      "8     wind_speed     int64       16.45              0.00\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/wildfires.txt\", delimiter='\\t')\n",
    "\n",
    "target, cols = feature_selection(df)\n",
    "\n",
    "\"\"\"for i in cols:\n",
    "    df[i]= normalise(df, i)\"\"\"\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(df[cols], df[target], test_size=0.3, random_state=10)\n",
    "\n",
    "    \n",
    "dataset = [X_train, X_test, y_train, y_test]\n",
    "for i in dataset:\n",
    "    i.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "X_train = np.hstack([X_train, np.ones(X_train.shape[0]).reshape(-1,1)])\n",
    "X_test = np.hstack([X_test, np.ones(X_test.shape[0]).reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_svm = e_Support_Vector_Machine(learning_rate= 1e-3, n_iter = 1000, tolerance =0.1, C=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0.  F1 score: 1.0\n",
      "Iteration: 1.  F1 score: 0.6153846153846154\n",
      "Iteration: 2.  F1 score: 0.888888888888889\n",
      "Iteration: 3.  F1 score: 0.9523809523809523\n",
      "Iteration: 4.  F1 score: 0.6153846153846153\n",
      "Iteration: 5.  F1 score: 0.8235294117647058\n",
      "Iteration: 6.  F1 score: 1.0\n",
      "Iteration: 7.  F1 score: 0.9411764705882353\n",
      "Iteration: 8.  F1 score: 0.8\n",
      "Iteration: 9.  F1 score: 0.5\n",
      "Mean F1 is: 0.8136744954392012\n"
     ]
    }
   ],
   "source": [
    "cross_val(my_svm, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00771738, -0.02422894,  0.03676193,  0.03794952,  0.00037754])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_y_train_predictions = my_svm.predict(X_train)\n",
    "my_y_predictions = my_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.888888888888889"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_svm_score = f1_score(y_test, my_y_predictions)\n",
    "my_svm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "skl_svm = svm.fit(X_train, y_train)\n",
    "skl_y_train_predictions = skl_svm.predict(X_train)\n",
    "skl_y_predicitons = skl_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709677419354839"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, skl_y_predicitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fp(actual, predicions):\n",
    "    tp=0\n",
    "    fp=0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i]==predicions[i]==1:\n",
    "            tp+=1\n",
    "        elif (actual[i]==1) & (predicions[i]==-1):\n",
    "            fp+=1\n",
    "    return tp,fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp = tp_fp(y_test, my_y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(len(y_test)):\n",
    "    fpr, tpr, _ = roc_curve(y_test, my_y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.06896552, 1.        ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.84848485, 1.        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = str(list(zip(y_test,my_y_predictions)))\n",
    "with open('Output.txt', 'w') as f:\n",
    "    f.writelines(output)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
