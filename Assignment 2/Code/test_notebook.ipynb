{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "import pdb\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from math import log2\n",
    "from sklearn.svm import SVC\n",
    "import itertools \n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "class Support_Vector_Machine(BaseEstimator):\n",
    "    \"\"\" This is my implimentation of a Support Vector Machine. \n",
    "    This can be used to for binary classification into positive and negative classes.\n",
    "\n",
    "    I am using the sci-kit learn project template as the structure for this class.\n",
    "    \n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    learning_rate : A parameter used to multiple by the cost gradient when iterating to control the step size.\n",
    "    n_iter : Determines how many iterations the gradient will be calculated until it meets an accectable tolerance \n",
    "    tolerance :Once the previous_cost-new_cost is within the range new_cost-tolerance the model will stop fitting and return the weights W\n",
    "    C : Regularisation hyperparameter used to control the 'softness' of the margin\n",
    "    lam : lambda value for regularisation in the cost function to penalise high values for W\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    X_ : ndarray, shape (n_samples, n_features)\n",
    "        The input passed during :meth:`fit`.\n",
    "    y_ : ndarray, shape (n_samples,)\n",
    "        The labels passed during :meth:`fit`.\n",
    "    classes_ : ndarray, shape (n_classes,)\n",
    "        The classes seen at :meth:`fit`.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=100e-6, n_iter=1e+3, tolerance=10e-3 , C=50e-3, lam=0.5, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.tolerance = tolerance\n",
    "        self.C = C\n",
    "        self.lam=lam\n",
    "        self.verbose = verbose\n",
    "        \n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Training an SVM is an optimisation problem where the objective is to maximise the distance between a hyperplane and the closest datapoint for any weight and bias (W, b).\n",
    "        The equation for a seperating hyperplane is (W).(X) + b = 0 where W is a weight vector, X is the input vector and b is a bias term.\n",
    "                    \n",
    "        The distance between the hyperplanes is 2/||W|| - so the goal is to minimise the magnitude of W and therefore maximise the \n",
    "        distnce between the hyperplanes with the condition no points lie between the two hyperplanes.\n",
    "        This minimising variable can be re-written as (||W||^2)*(1/2) for convenience, it is now a quadratic with one global minimum.\n",
    "        \n",
    "        So writing the eq. for hyperplanes for the two classes gives:\n",
    "             (W).(Xi) + b = 1 for positive class (yi=1)\n",
    "             (W).(Xi) + b = -1 for negative class (yi=-1)\n",
    "        This can be generalised for both classes as yi is known and can be multiplied by the above equations to give the form:\n",
    "            yi(Xi.W+b)-1=0\n",
    "\n",
    "        We want to minimise ||W|| and maximise b by iterating through possible values for W and keeping the W and b that satisfy the above equation \n",
    "        and picking the W and b that minimise ||W||. This will be done using Stochastic Gradient Descent and the bias term will be evaluated in the W vector.\n",
    "\n",
    "        The cost funciton is described by the equation:\n",
    "        J= (||W||^2)/2  +  (C/N)*SUMALL(maxvalue(0, 1-yi*W*xi)) \n",
    "\n",
    "        The gradient of this cost funciton is :\n",
    "        DJ = 1/N SUMALL(w) if the max(0, 1-yi*W*xi) = 0 and satisfies the general form of the hyperplane equation or\n",
    "        DJ = 1/N SUMALL(w-C*yi*xi) where is does not satify the eqn.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The training input samples.\n",
    "        y : The target values. An array of int.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : Returns self.W \n",
    "        \"\"\"\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        # Initialise the weight vector with length the same as the number of features being analysed and give each coefficient a value of 1.\n",
    "        W = np.ones(X.shape[1])  \n",
    "\n",
    "        # For each iteration, calculate the weights.\n",
    "        for step in range(self.n_iter):\n",
    "            # For each X value evaluate the gradient at that point with the given weights and subtract the gradient*learning rate from the weights to refine the W vector\n",
    "            for index, value in enumerate(X):\n",
    "                W = W-self.learning_rate*gradient(W, value,  y[index], self.C)\n",
    "        self.W = W\n",
    "        return(self.W)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Calculate the dot product of transpose(W) and Xi. If the result is a positive number then assign it the positive class, vice versa for the negative class\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : The input samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : The label for each sample either 1 or -1 depending on the sign of the dot product of W.T and Xi\n",
    "         \"\"\"\n",
    "        # Check is fit had been called\n",
    "        #check_is_fitted(self, ['X_', 'y_'])\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        output = []\n",
    "        for i in X:\n",
    "            if np.dot(self.W.T,i)>0:\n",
    "                output.append(1)\n",
    "            else:\n",
    "                output.append(-1)\n",
    "        return(output)\n",
    "    \n",
    "   \n",
    "def information_gain(df, target, columns):\n",
    "    \"\"\"\n",
    "    Calculate the information gain for all the columns to be presented at the feature selection screen. \n",
    "    Mean value will be used to bucket the values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : The input dataframe.\n",
    "    target : The chosen target variable.\n",
    "    columns : The chosen independant variables\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_output: datafame, shape (n_columns, 3) where columns are:\n",
    "    Column : Column of the dataframe\n",
    "    Mean Value : mean value of the independant variable\n",
    "    Information Gain: Information Gain given by the variable binned above and below the mean variable value. \n",
    "\n",
    "    \"\"\"\n",
    "    df_output = pd.DataFrame()\n",
    "    \n",
    "    target_vals = list(set(df[target]))\n",
    "    val1 = target_vals[0]\n",
    "    val2 = target_vals[1]\n",
    "    \n",
    "    df_entropy = -(len(df[df[target]==val1])/len(df))*log2(len(df[df[target]==val1])/len(df)) - (len(df[df[target]==val2])/len(df))*log2(len(df[df[target]==val2])/len(df))\n",
    "    \n",
    "    \n",
    "    for col in columns:        \n",
    "        mean_val = np.mean(df[col])\n",
    "        high_val= df[df[col]>=mean_val]\n",
    "        low_val = df[df[col]<mean_val]\n",
    "        try:\n",
    "            # Some columns like rain have no fires above the mean value so the below equation breaks down - this is a very significant feature to include\n",
    "            if len(set(high_val[target])) ==2 :\n",
    "                high_exp1 = -(len(high_val[high_val[target]==val1]) / len(high_val))*log2(len(high_val[high_val[target]==val1])/len(high_val)) \n",
    "                high_exp2 = -(len(high_val[high_val[target]==val2]) / len(high_val))*log2(len(high_val[high_val[target]==val2])/len(high_val))\n",
    "                high_ent =  high_exp1 + high_exp2\n",
    "            else:\n",
    "                high_ent=0\n",
    "\n",
    "            if len(set(low_val[target])) ==2 :\n",
    "                low_exp1 = -(len(low_val[low_val[target]==val1]) / len(low_val))*log2(len(low_val[low_val[target]==val1])/len(low_val)) \n",
    "                low_exp2 = -(len(low_val[low_val[target]==val2]) / len(low_val))*log2(len(low_val[low_val[target]==val2])/len(low_val))\n",
    "                low_ent =  low_exp1 + low_exp2\n",
    "            else:\n",
    "                low_ent=0\n",
    "\n",
    "            info_gain = df_entropy - (len(high_val)/len(df))*high_ent - (len(low_val)/len(df))*low_ent\n",
    "            df_output = df_output.append([[col,np.round(mean_val,2),np.round(info_gain, 2)]])\n",
    "        except:\n",
    "            pass\n",
    "    df_output = df_output.rename(columns={0:'Column', 1:\"Mean Value\", 2:\"Information Gain\"})\n",
    "    return(df_output)\n",
    "        \n",
    "        \n",
    "\n",
    "def feature_selection(df):\n",
    "    \"\"\"\n",
    "    Allows user input to pick the dependant and independant variables. \n",
    "    Once the dependant variable is chosen the information gain for the independant variables is calculated to help the user pick out useful features. \n",
    "    To calculate information gain variables were binned according to the feature mean value \n",
    "        - this is not useful in the case of categorical data but the user should know that, this is just to assist the feature selection process.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : The input dataframe.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_output: datafame, shape (n_columns, 3) where columns are:\n",
    "    Column : Column of the dataframe\n",
    "    Data Type: Column showing the dtype of the independant variables\n",
    "    Mean Value : mean value of the independant variable\n",
    "    Information Gain: Information Gain given by the variable binned above and below the mean variable value. \n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(df.dtypes)\n",
    "    #target='yes'\n",
    "    target = input('Pick the target variable: ')\n",
    "    \n",
    "    df[target] = [x.strip() for x in df[target]]\n",
    "    df[target] = df[target].replace({'no':-1, 'yes':1})\n",
    "    df_cols = df.drop(target ,axis=1)\n",
    "   \n",
    "    ig = information_gain(df, target, df_cols.columns)\n",
    "    info_cols = pd.DataFrame(df_cols.dtypes)\n",
    "    info_cols.reset_index(inplace=True)\n",
    "    info_cols = info_cols.rename(columns={'index':'Column', 0:'Data Type'})\n",
    "    info_cols= info_cols.merge(ig, on='Column').sort_values(\"Information Gain\" ,ascending=False)\n",
    "    print(\"\\n\")\n",
    "    print(\"Information gain calculated for bins either side of mean values for each feature\")\n",
    "    print(info_cols)\n",
    "    #cols = \"rainfall, humidity, buildup_index, drought_code\"\n",
    "    cols = input(\"Please enter the desired columns for anaylsis (use a comma seperate the features): \")\n",
    "    cols = [x.strip() for x in cols.split(',')]\n",
    "    return target, cols\n",
    "\n",
    "def normalise(df, column):\n",
    "    \"\"\"\n",
    "    Function to normalise the data in the datasets columns - \n",
    "    -1, 1 normalising is done through applying 2 * (x-min(x) / (max(x)-min(x)\n",
    "    This has a negative impact on the models performance but included because was covered in lectures and to show work done.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : The input dataframe.\n",
    "\n",
    "    column : Column of the dataframe to be normalised\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Normlised series\n",
    "\n",
    "    \"\"\"\n",
    "    return 2*(df[column]-min(df[column]))/(max(df[column]) - min(df[column]))-1\n",
    "    \n",
    "\n",
    "def cost_function(W, X, y, C, lam):\n",
    "    \"\"\"\n",
    "    The cost funciton is described by the equation below and will be evaluated to determine if the model has achieved an acceptably \n",
    "    low cost function before the number of iterations has been reached.\n",
    "    A complexity penalty has been added too to force the model to favour low weight values in W\n",
    "    \n",
    "    J(W) = emperical_cost(W) + lambdaComplexity(W)\n",
    "\n",
    "    J(W)= 1/2(W.T*W)  +  (C/N)*SUMALL(maxvalue(0, 1-yi*W*xi)) + lambda*Complexity(|W|)\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    W : Stores the current weights of the model\n",
    "    X : Xi values for the given point\n",
    "    y : dependant variable for Xi\n",
    "    C: regularisation hyper parameter for tuning the soft margin strength\n",
    "    lam: regularisation hyper parameter for penalising large W values\n",
    "    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    computed cost of the resulting W values for a given Xi\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(len(X)):\n",
    "        \n",
    "        N = len(X)\n",
    "        # Evaluate for the left side of the '+'. Dot product of a vector on itself returns the magnitude\n",
    "        hyper_plane_distance = np.max([0,1-(y[i]*np.dot(X[i],W.T))])\n",
    "\n",
    "        emperical_cost = (1/2) * np.dot(W.T,W) + (C/N)*np.sum(hyper_plane_distance)\n",
    "        complexity = lam*np.linalg.norm(W)\n",
    "        return (emperical_cost+complexity)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def gradient(W, X, y, C):\n",
    "    '''\n",
    "    Calculate the hinge loss at a point for given values of W, Xi, yi and return the value for the SVM to evaluate the next values for W if HL!=0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    W :1D numpy array, shape (n_features)\n",
    "            Stores the current weights of the model\n",
    "    X : Xi values for the given point\n",
    "    y : dependant variable for Xi\n",
    "    C: regularisation hyper parameter for tuning the soft margin strength\n",
    "  \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grad: New set of weights to be used in the next iteration of the model\n",
    "\n",
    "    '''\n",
    "    \n",
    "    grad = np.zeros(len(W))\n",
    "    # Calculate hinge loss the point at  Xi, yi using W. If less than 0,  assign 0.\n",
    "    hl = np.max([0, 1 - y * np.dot(W.T,X)])\n",
    "    \n",
    "    # If the max value of the point is 0 then loss is minimised for thie W, return it to the model\n",
    "    if hl == 0:\n",
    "        grad = W\n",
    "    else:\n",
    "        # HL has not been minimised yet, return this W to the model to be iterated over next time\n",
    "        grad = W - (C * y * X)\n",
    "    return grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cross_val(clf, X_, y_, n_iter):\n",
    "    \n",
    "    \"\"\"\n",
    "    SK Learns cross_val_score was not working with my implimentation of the SVM so the below code shuffles and splits the dataset into 2/3 and 1/3 for training and validation. \n",
    "    For every step in n_iter, the validation set will iterate through the 3 folds and a model will be fitted on the training data and evaluated on the validation set.\n",
    "    The first 1/3 elements are taken for validation and the remainder are training. \n",
    "\n",
    "    Once the first 1/3 items have been used for validaiton they are concatenated onto the end of the training set and these first 1/3 elements are removed from the top of the training set.\n",
    "    In the next loop a fresh 1/3 of the dataset will be used for vlalidaiton.\n",
    "\n",
    "    Drawbacks of this function is a lack of stratified sampling - Some validation sets may have an unrepresentative quantity of a particular class. \n",
    "    While not ideal, this is a known draw back and can be lived with.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf : the chosen classifier - must be compatible with the sci-kit learn API for fit() and predict()\n",
    "    X : X values \n",
    "    y : y values\n",
    "    n_iter: how many iterations are needed\n",
    "  \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Nothing\n",
    "\n",
    "    Will print out the iteraion, validaion fold and f1 score for each model being evaluated. A mean F1 score is calculated at the end.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    output_scores=[]\n",
    "    \n",
    "    for step in range(n_iter):\n",
    "        # Shuffle the dataset every step in the iteration\n",
    "        X,y = shuffle(X_,y_)\n",
    "        # For each fold in the iteraion\n",
    "        for fold in range(3):\n",
    "            # Take the top 1/3 of the dataset for validation, rest for training\n",
    "            index_slicer = len(X)//3\n",
    "            X_val, y_val = X[ :index_slicer ], y[ : index_slicer]\n",
    "            X_train, y_train = X[index_slicer: ], y[index_slicer: ]\n",
    "\n",
    "            # Append the first 1/3elements to the end of the array and then slice them off the start.\n",
    "            # By always treating the first j elements as the validation set and the remainder as the training set, \n",
    "            # I can do n-fold CV without adapting my Support_Vector_machine class to work with sci kit learns cv.\n",
    "\n",
    "            X, y =np.concatenate((X_train,X_val)),np.concatenate((y_train,y_val))\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            clf.fit(X_train, y_train)\n",
    "            clf_predicts = clf.predict(X_val)\n",
    "            f1 = f1_score(y_val, clf_predicts)\n",
    "            print(f'Iteration: {step}. Fold: {fold}  F1 score: {f1}')\n",
    "            output_scores.append(f1)\n",
    "    #pdb.set_trace()\n",
    "    print(f'Mean F1 is: {np.mean(output_scores)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes               object\n",
      "year               int64\n",
      "temp               int64\n",
      "humidity           int64\n",
      "rainfall         float64\n",
      "drought_code     float64\n",
      "buildup_index    float64\n",
      "day                int64\n",
      "month              int64\n",
      "wind_speed         int64\n",
      "dtype: object\n",
      "Information gain calculated for bins either side of mean values for each feature\n",
      "          Column Data Type  Mean Value  Information Gain\n",
      "5  buildup_index   float64       16.54              0.30\n",
      "3       rainfall   float64        0.82              0.25\n",
      "4   drought_code   float64       48.54              0.23\n",
      "1           temp     int64       31.91              0.18\n",
      "2       humidity     int64       62.28              0.11\n",
      "6            day     int64       15.69              0.03\n",
      "7          month     int64        7.55              0.03\n",
      "0           year     int64     2011.98              0.00\n",
      "8     wind_speed     int64       16.45              0.00\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/wildfires.txt\", delimiter='\\t')\n",
    "\n",
    "target, cols = feature_selection(df)\n",
    "\n",
    "\"\"\"for i in cols:\n",
    "    df[i]= normalise(df, i)\"\"\"\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(df[cols], df[target], test_size=0.3, random_state=10)\n",
    "\n",
    "    \n",
    "dataset = [X_train, X_test, y_train, y_test]\n",
    "for i in dataset:\n",
    "    i.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "X_train = np.hstack([X_train, np.ones(X_train.shape[0]).reshape(-1,1)])\n",
    "X_test = np.hstack([X_test, np.ones(X_test.shape[0]).reshape(-1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_svm = e_Support_Vector_Machine(learning_rate= 1e-3, n_iter = 10, tolerance =1e-12, C=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0.  F1 score: 0.7272727272727272\n",
      "Iteration: 1.  F1 score: 1.0\n",
      "Iteration: 2.  F1 score: 0.6666666666666666\n",
      "Iteration: 3.  F1 score: 0.8571428571428571\n",
      "Iteration: 4.  F1 score: 0.6666666666666666\n",
      "Iteration: 5.  F1 score: 0.8235294117647058\n",
      "Iteration: 6.  F1 score: 0.7142857142857143\n",
      "Iteration: 7.  F1 score: 0.8571428571428571\n",
      "Iteration: 8.  F1 score: 0.9333333333333333\n",
      "Iteration: 9.  F1 score: 0.888888888888889\n",
      "Mean F1 is: 0.8134929123164417\n"
     ]
    }
   ],
   "source": [
    "cross_val(my_svm, X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_y_train_predictions = my_svm.predict(X_train)\n",
    "my_y_predictions = my_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_svm_score = f1_score(y_test, my_y_predictions)\n",
    "my_svm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "skl_svm = svm.fit(X_train, y_train)\n",
    "skl_y_train_predictions = skl_svm.predict(X_train)\n",
    "skl_y_predicitons = skl_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, skl_y_predicitons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_fp(actual, predicions):\n",
    "    tp=0\n",
    "    fp=0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i]==predicions[i]==1:\n",
    "            tp+=1\n",
    "        elif (actual[i]==1) & (predicions[i]==-1):\n",
    "            fp+=1\n",
    "    return tp,fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fp = tp_fp(y_test, my_y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(len(y_test)):\n",
    "    fpr, tpr, _ = roc_curve(y_test, my_y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = str(list(zip(y_test,my_y_predictions)))\n",
    "with open('Output.txt', 'w') as f:\n",
    "    f.writelines(output)\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "50e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val2(clf, X, y, n_iter):\n",
    "    \n",
    "    \"\"\"\n",
    "    SK Learns cross_val_score was not working with my implimentation of the SVM so the below code shuffles and splits the dataset into 2/3 and 1/3 for training and validation. \n",
    "    For every step in n_folds, the validation set will iterate through the 3 \n",
    "    The first j elements are taken for validation and the remainder are training. Once the first j items have been used for validaiton they are concatenated onto the end of the training set and the next j elements are taken from the top of the training set.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    output_scores=[]\n",
    "    \n",
    "    for step in range(n_iter):\n",
    "        X,y = shuffle(X,y)\n",
    "        for fold in range(3):\n",
    "            index_slicer = len(X)//3\n",
    "            X_val, y_val = X[ :index_slicer ], y[ : index_slicer]\n",
    "            X_train, y_train = X[index_slicer: ], y[index_slicer: ]\n",
    "\n",
    "            # To iterate through the folds of the cross validation, append the first j elements to the end of the array and then slice them off the start.\n",
    "            # By always treating the first j elements as the validation set and the remainder as the training set, I can do n-fold CV without adapting my e_Support_Vector_machine class to accecpt the sklearn implimentation.\n",
    "\n",
    "            X, y =np.concatenate((X_train,X_val)),np.concatenate((y_train,y_val))\n",
    "            #X, y = X[index_slicer: ], y[index_slicer :] \n",
    "\n",
    "            #pdb.set_trace()\n",
    "\n",
    "            clf.fit(X_train, y_train)\n",
    "            clf_predicts = clf.predict(X_val)\n",
    "            f1 = f1_score(y_val, clf_predicts)\n",
    "            print(f'Iteration: {step}. Fold: {fold}  F1 score: {f1}')\n",
    "            output_scores.append(f1)\n",
    "    #pdb.set_trace()\n",
    "    print(f'Mean F1 is: {np.mean(output_scores)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [{'learning_rate':[10e-3, 1e-3, 100e-6], 'n_iter':[1000, 5000, 10000], 'tolerance':[1e-3], 'C':[0.05, 0.1, 0.2]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_svm = Support_Vector_Machine(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(my_svm, param_grid, cv=10, scoring='f1', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
